<!DOCTYPE html>
<html>
<head>
    <meta charset="utf-8">
    <meta name="description" content="Kangaroo: Video LLM.">
    <meta name="keywords" content="Multi-modal, Video, LLM">
    <meta name="viewport" content="width=device-width, initial-scale=1">
    <title>Kangaroo</title>

    <style>
        td{border:1px solid #cacaca;font-size:15px;}
        th{border:1px solid #cacaca;font-size:16px;}
		footer{text-align: center}
        .gg{vertical-align: middle; background:rgb(242, 242, 242);}
        .lgg{vertical-align: middle; background:rgb(250, 250, 250);}
        .wg{vertical-align: middle;}
        .video-container{display: flex; flex-direction: row; align-items: center; justify-content: center;}
    </style>

    <link rel="stylesheet" href="https://fonts.googleapis.com/css?family=Google+Sans|Noto+Sans|Castoro">
    <link rel="stylesheet" href="./static/css/bulma.min.css">
    <link rel="stylesheet" href="./static/css/bulma-carousel.min.css">
    <link rel="stylesheet" href="./static/css/bulma-slider.min.css">
    <link rel="stylesheet" href="./static/css/fontawesome.all.min.css">
    <link rel="stylesheet" href="https://cdn.jsdelivr.net/gh/jpswalsh/academicons@1/css/academicons.min.css">
    <link rel="icon" href="./static/images/favicon.svg">

    <script src="https://ajax.googleapis.com/ajax/libs/jquery/3.5.1/jquery.min.js"></script>
    <script src="./static/js/bulma-carousel.min.js"></script>
    <script src="./static/js/bulma-slider.min.js"></script>
    <script src="./static/js/fontawesome.all.min.js"></script>
</head>
<body>

<!-- Title -->
<section class="hero">
	<div class="hero-body">
		<div class="container is-max-desktop">
			<div class="columns is-centered">
				<div class="column has-text-centered">
					<h1 class="title is-1 publication-title" style="display: flex; align-items: center;justify-content: center;font-size: 46px;width: 1100px;">
					<img src="static/images/kangaroo_logo.png" alt="Logo" style="height: 100px;">
					Kangaroo: A Powerful Video-Language Model Supporting Long-context Video Input</h1>
					
					<div class="is-size-5 publication-authors">
						<span class="author-block">
						Jiajun Liu<sup>1*</sup>,</span>
						<span class="author-block">
						Yibing Wang<sup>1,2*</sup>,</span>
						<span class="author-block">
						Hanghang Ma<sup>1&dagger;</sup>,
						</span>
						<span class="author-block">
						Xiaoping Wu<sup>1&dagger;</sup>,
						</span>
						<span class="author-block">
						Xiaoqi Ma<sup>1&dagger;</sup>,
						</span>
						<span class="author-block">
						<a href="https://scholar.google.com/citations?user=DAJdHnkAAAAJ&hl=en"> Jie Hu</a><sup>1&ddagger;</sup>
						</span>
					</div>

					<div class="is-size-6 publication-authors" style="margin-top: 10px;">
						<sup>1 </sup>Meituan &nbsp;&nbsp;<sup>2 </sup>University of Chinese Academy of Sciences
					</div>

					<div class="is-size-6 publication-authors" style="margin-bottom: 20px;margin-top: 5px;">
						<sup>* </sup>Joint first authors &nbsp;&nbsp; <sup>&dagger; </sup>Key contributors &nbsp;&nbsp; 
						<sup>&ddagger; </sup>Project lead & <a href="mailto:hujie@ios.ac.cn"> Corresponding author </a>
					</div>

					<div class="column has-text-centered">
						<!-- PDF Link. -->
						<span class="link-block">
							<!-- <a href="https://arxiv.org/pdf/2011.12948" -->
							<a class="external-link button is-normal is-rounded is-dark">
							<span class="icon">
								<i class="fas fa-file-pdf"></i>
							</span>
							<span>Paper (coming soon)</span>
							</a>
						</span>

						<!-- Code Link. -->
						<span class="link-block">
							<a href="https://github.com/KangarooGroup/Kangaroo"
							class="external-link button is-normal is-rounded is-dark">
							<span class="icon">
								<i class="fab fa-github"></i>
							</span>
							<span>Code</span>
							</a>
						</span>

						<!-- Model Link. -->
						<span class="link-block">
							<a href="https://huggingface.co/KangarooGroup/kangaroo"
							class="external-link button is-normal is-rounded is-dark">
							<span class="icon">
								ðŸ¤—
							</span>
							<span>Model</span>
							</a>
						</span>
        	</div>
      	</div>
    	</div>
  	</div>
	</div>
</section>

<!-- Abstract. -->
<section class="section">
	<div class="container is-max-desktop">
		<div class="columns is-centered has-text-centered">
			<div class="column is-four-fifths">
				<h2 class="title is-3">Abstract</h2>
				<div class="content has-text-justified">
					<p>
						We introduce <strong>Kangaroo</strong>, a powerful Multimodal Large Language Model designed for long-context video understanding. 
                        Our presented Kangaroo model shows remarkable performance across diverse video understanding tasks including video caption, QA 
                        and conversation. Generally, our key contributions in this work can be summarized as follows:
					<ol>
						<li><strong>Long-context Video Input.</strong>
						To enhance the model's capability to comprehend longer videos, we extend the maximum frames of input videos to 160. To this end, we 
                        aggregate multiple videos with variable frame counts and aspect ratios into one sample. We further design a spatial-temporal pathify 
                        module to improve training efficiency.</li>
						<li><strong>Strong Performance.</strong>
						We evaluate our model across various video understanding benchmarks. The results indicate that our model achieves state-of-the-art 
						performance on the majority of comprehensive benchmarks and maintain a competitive level in others. Notably, our model outperforms 
						most larger open-source models with over 30B parameters and some proprietary models on certain benchmarks.</li>
						<li><strong>Video Annotation System.</strong>
                        We develop a data curation and automatic annotation system to generate captions for open-source and internal videos. The generated 
                        large-scale dataset are utilized for video-text pre-training. For video instruction tuning stage, we construct a video instruciton 
                        tuning dataset based on public and internal datasets covering a variety of tasks.</li>
                        <li><strong>Billingual Conversation.</strong>
						Our proposed model is equipped with the capability of Chinese, English and billingual conversations, and support single/multi-round 
                        conversation paradigms.</li>
					</ol>
				</div>
			</div>
		</div>
</section>

<!-- Model -->
<section class="section">
	<div class="container is-max-desktop">
		<div class="columns is-centered has-text-centered">
			<div class="column is-four-fifths">
				<h2 class="title is-3" style="margin-bottom: 20px">Model</h2>
			</div>
		</div>
	</div>
	<div style="text-align: center;">
        <img id="model" width="60%" src="static/images/model.png">
        <h3 class="subtitle has-text-centered">
        <p style="font-family:Times New Roman; font-size: 18px;">Architecture of our Kangaroo model.</p>
	</div>
</section>

<!-- Dataset -->
<section class="section">
	<div class="container is-max-desktop">
		<div class="columns is-centered has-text-centered">
			<div class="column is-four-fifths">
				<h2 class="title is-3">Dataset Statistics</h2>
				<div class="content has-text-justified">
					<p><strong style="font-size: 20px;">Data Curation</strong><br>
						We select 300M images from LAION, COYO and Wukong for image-text alignment and 60M videos from Webvid, Youku-mPLUG and internal social media short 
                        videos for video-text alignment, following a category balance strategy. Images and videos with excessive text coverage, significant face coverage 
                        and low optical flow scores (for videos) are filtered out. 
					</p>
					<p><strong style="font-size: 20px;">Automatic Annotation</strong><br>
						We develop an automatic video annotation system. The process begins with the extraction of five key frames of each video, followed by the utilization 
                        of three distinct off-the-shelf Multimodal Large Language Models (MLLM) to generate frame captions. Next, we employ an LLM to synthesize the key 
                        frame captions into a comprehensive video caption. We further curate a subset of 6M pre-training data and incorporate 900K dense caption data from 
                        ShareGPTVideo dataset for the refined pre-training stage.
					</p>
					<p><strong style="font-size: 20px;">Instruction Tuning Dataset</strong><br>
						To enhance the instruction following ability of the model, we compile a video instruction tuning dataset comprising 2.24M samples from public and 
                        internal sources. The dataset consists of short caption, detailed description, multi-choice QA, open-ended QA, single/multi-round conversation 
                        in both Chinese and English.
				</div>
			</div>
		</div>
		<div class="image-row" style="text-align: center;">
			<img src="static/images/instruction_dataset.png" width="35%" style="margin: 20px;">
			<h3 class="subtitle has-text-centered">
			<p style="font-family:Times New Roman; font-size: 18px;">Distribution of instruction tuning dataset.</p>
		</div>
	</div>
</section>

<!-- Model Card -->
<section class="section">
	<div class="container is-max-desktop">
	<div class="columns is-centered has-text-centered">
		<div class="column is-four-fifths">
			<h2 class="title is-3">Model Card</h2>
		</div>
	</div>
	</div>
	<div style="text-align: center; margin: 40px;">
		<table style="text-align: center;;margin:auto">
			<colgroup>
			<col style="mso-width-source:userset;width:150pt"> 
			<col style="mso-width-source:userset;width:160pt"> 
			<col style="mso-width-source:userset;width:100pt"> 
			<col style="mso-width-source:userset;width:180pt">
			<col style="mso-width-source:userset;width:120pt">
			</colgroup>
			<tbody>
			<tr height="45"> 
			<!-- <th rowspan="2" class="gg">Model</th>  -->
			<th rowspan="2" style="font-size: 20px;" class="gg">Modules</th> 
			<th class="gg">Visual Encoder</th> 
			<th class="gg">Projector</th> 
			<th class="gg">Patchify Module</th> 
			<th class="gg">LLM</th> 
			</tr> 
			<tr height="45"> 
			<td class="lgg">EVA-CLIP ViT-L</td> 
			<td class="lgg">Linear</td> 
			<td class="lgg">3D Depthwise convolution</td> 
			<td class="lgg">Llama3-8B</td> 

			<tr height="45"> 
			<th colspan="1" rowspan="3" class="gg" style="font-size: 18px;">Stage 1<br>Image Pre-training</th> 
			<th class="wg">Resolution</th> 
			<td colspan="3" class="wg">224</td> 
			</tr> 
			<tr height="45"> 
			<th class="lgg">Training Data</th> 
			<td colspan="3" class="lgg">300M</td> 
			</tr> 
			<tr height="45"> 
			<th class="wg">Trainable Module</th> 
			<td colspan="3" class="wg">ViT + Projector</td> 
			</tr> 
			<tr height="45"> 
			<th colspan="1" rowspan="3" class="gg" style="font-size: 18px;">Stage 2<br>Video Pre-training</th> 
			<th class="lgg">Resolution</th> 
			<td colspan="3" class="lgg">224 Ã— 8 frames</td> 
			</tr> 
			<tr height="45"> 
			<th class="wg">Training Data</th> 
			<td colspan="3" class="wg">60M</td> 
			</tr> 
			<tr height="45"> 
			<th class="lgg">Trainable Module</th> 
			<td colspan="3" class="lgg">ViT + Projector</td> 
			</tr> 
			<tr height="45"> 
			<th colspan="1" rowspan="3" class="gg" style="font-size: 18px;">Stage 3<br>Refined Pre-training</th> 
			<th class="wg">Resolution</th> 
			<td colspan="3" class="wg">448 Ã— 16 frames</td> 
			</tr> 
			<tr height="45"> 
			<th class="lgg">Training Data</th> 
			<td colspan="3" class="lgg">6.9M</td> 
			</tr> 
			<tr height="45"> 
			<th class="wg">Trainable Module</th> 
			<td colspan="3" class="wg">ViT + Projector + Patchify Module</td> 
			</tr> 
			<tr height="45"> 
			<th colspan="1" rowspan="3" class="gg" style="font-size: 18px;">Stage 4<br>Instruction Tuning</th> 
			<th class="lgg">Resolution</th> 
			<td colspan="3" class="lgg">448 Ã— (16 to 64 frames)</td> 
			</tr> 
			<tr height="45"> 
			<th class="wg">Training Data</th> 
			<td colspan="3" class="wg">2.24M</td> 
			</tr> 
			<tr height="45"> 
			<th class="lgg">Trainable Module</th> 
			<td colspan="3" class="lgg">Full model</td> 
			</tr> 
			<tr height="45"> 
			<th colspan="1" rowspan="3" class="gg" style="font-size: 18px;">Stage 5<br>Long Video Tuning</th> 
			<th class="wg">Resolution</th> 
			<td colspan="3" class="wg">448 Ã— (16 to 64 frames for short videos, 64 to 160 frames for long videos)</td> 
			</tr> 
			<tr height="45"> 
			<th class="lgg">Training Data</th> 
			<td colspan="3" class="lgg">700K</td> 
			</tr> 
			<tr height="45"> 
			<th class="wg">Trainable Module</th> 
			<td colspan="3" class="wg">Projector + Patchify Module + LLM</td> 
			</tr> <!--EndFragment--> 
			</tbody>
		</table>
	</div>
</section>

<!-- Result -->
<section class="section">
	<div class="container is-max-desktop">
		<div class="columns is-centered has-text-centered">
			<div class="column is-four-fifths">
				<h2 class="title is-3">Results</h2>
			</div>
		</div>
	</div>
	<div style="text-align: center;">
		<img src="static/images/bench.png" width="70%" style="margin-top: 40px;">
	</div>
</section>

<!-- Demo -->
<section class="section">
	<div style="text-align: center;">
		<h2 class="title is-3">Demo</h2>
		<p style="font-family:Times New Roman;font-size:larger;">
		(Demo videos are from <a href=https://openai.com/index/video-generation-models-as-world-simulators>openai sora)</a></p>
		
		<!-- Demo1 -->
		<div class="video-container" style="text-align: center; margin: 10px;">
			<video id="demo1_1" controls muted loop playsinline style="height: 200px; margin: 20px">
				<source src="./static/videos/demo1_1.mp4" type="video/mp4">
			</video>
			<video id="demo1_2" controls muted loop playsinline style="height: 200px; margin: 20px">
				<source src="./static/videos/demo1_2.mp4" type="video/mp4">
			</video>
		</div>
		<img id="model" width="70%" src="static/images/demo1.png">
	</div>

	<!-- Demo2 -->
	<div style="text-align: center; margin: 100px;">
		<div class="video-container" style="text-align: center; margin: 50px;">
			<video id="demo2" controls muted loop playsinline style="height: 250px;">
				<source src="./static/videos/demo2.mp4" type="video/mp4">
			</video>
		</div>
		<img id="model" width="80%" src="static/images/demo2.png">
	</div>
</section>

<!-- Citation -->
<section class="section">
	<h2 class="title is-3" style="text-align: center;">Citation</h2>
    <pre style="width: 850px; margin: auto;"><code>
        @misc{kangaroogroup,
            title={Kangaroo: A Powerful Video-Language Model Supporting Long-context Video Input},
            url={https://kangaroogroup.github.io/Kangaroo.github.io/},
            author={Jiajun Liu and Yibing Wang and Hanghang Ma and Xiaoping Wu and Xiaoqi Ma and Jie Hu},
            month={July},
            year={2024}
        }
    </code></pre>
</section>

<!-- foot -->
<footer class="footer">
	<div class="columns is-centered">
		<div class="column is-4">
			<div class="content">
				<p style="font-size: larger;"><strong>Acknowledgments:</strong></p>
				<p>
					This website is adopted from the <a href="https://github.com/nerfies/nerfies.github.io">
					source code </a> of this website, licensed under a <a rel="license" href="http://creativecommons.org/licenses/by-sa/4.0/">
					Creative Commons Attribution-ShareAlike 4.0 International License</a>. Thanks for their excellent work.
				</p>
			</div>
		</div>
	</div>
</footer>

</body>
</html>
