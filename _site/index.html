<!DOCTYPE html>
<html>
<head>
    <meta charset="utf-8">
    <meta name="description" content="Kangaroo: Video LLM.">
    <meta name="keywords" content="Multi-modal, Video, LLM">
    <meta name="viewport" content="width=device-width, initial-scale=1">
    <title>Kangaroo</title>

    <style>
        td{border:1px solid #cacaca;font-size:15px;}
        th{border:1px solid #cacaca;font-size:16px;}
		footer{text-align: center}
        .gg{vertical-align: middle; background:rgb(242, 242, 242);}
        .lgg{vertical-align: middle; background:rgb(250, 250, 250);}
        .wg{vertical-align: middle;}
        .video-container{display: flex; flex-direction: row; align-items: center; justify-content: center;}
    </style>

    <link rel="stylesheet" href="https://fonts.googleapis.com/css?family=Google+Sans|Noto+Sans|Castoro">
    <link rel="stylesheet" href="./static/css/bulma.min.css">
    <link rel="stylesheet" href="./static/css/bulma-carousel.min.css">
    <link rel="stylesheet" href="./static/css/bulma-slider.min.css">
    <link rel="stylesheet" href="./static/css/fontawesome.all.min.css">
    <link rel="stylesheet" href="https://cdn.jsdelivr.net/gh/jpswalsh/academicons@1/css/academicons.min.css">
    <link rel="icon" href="./static/images/favicon.svg">

    <script src="https://ajax.googleapis.com/ajax/libs/jquery/3.5.1/jquery.min.js"></script>
    <script src="./static/js/bulma-carousel.min.js"></script>
    <script src="./static/js/bulma-slider.min.js"></script>
    <script src="./static/js/fontawesome.all.min.js"></script>
</head>
<body>

<!-- Title -->
<section class="hero">
	<div class="hero-body">
		<div class="container is-max-desktop">
			<div class="columns is-centered">
				<div class="column has-text-centered">
					<h1 class="title is-1 publication-title" style="display: flex; align-items: center;justify-content: center;font-size: 46px;width: 1100px;">
					<img src="static/images/kangaroo_logo.png" alt="Logo" style="height: 100px;">
					Kangaroo: A Powerful Video-Language Model Supporting Long-context Video Input</h1>
					
					<div class="is-size-5 publication-authors">
						<span class="author-block">
						Jiajun Liu<sup>1*</sup>,</span>
						<span class="author-block">
						Yibing Wang<sup>1,2*</sup>,</span>
						<span class="author-block">
						Hanghang Ma<sup>1&dagger;</sup>,
						</span>
						<span class="author-block">
						Xiaoping Wu<sup>1&dagger;</sup>,
						</span>
						<span class="author-block">
						Xiaoqi Ma<sup>1&dagger;</sup>,
						</span>
						<span class="author-block">
						<a href="https://scholar.google.com/citations?user=DAJdHnkAAAAJ&hl=en"> Jie Hu</a><sup>1&ddagger;</sup>
						</span>
					</div>

					<div class="is-size-6 publication-authors" style="margin-top: 10px;">
						<sup>1 </sup>Meituan &nbsp;&nbsp;<sup>2 </sup>University of Chinese Academy of Sciences
					</div>

					<div class="is-size-6 publication-authors" style="margin-bottom: 20px;margin-top: 5px;">
						<sup>* </sup>Joint first authors &nbsp;&nbsp; <sup>&dagger; </sup>Key contributors &nbsp;&nbsp; 
						<sup>&ddagger; </sup>Project lead & <a href="mailto:hujie@ios.ac.cn"> Corresponding author </a>
					</div>

					<div class="column has-text-centered">
						<!-- PDF Link. -->
						<span class="link-block">
							<a href="http://arxiv.org/abs/2408.15542"
							class="external-link button is-normal is-rounded is-dark">
							<span class="icon">
								<i class="fas fa-file-pdf"></i>
							</span>
							<span>Paper</span>
							</a>
						</span>

						<!-- Code Link. -->
						<span class="link-block">
							<a href="https://github.com/KangarooGroup/Kangaroo"
							class="external-link button is-normal is-rounded is-dark">
							<span class="icon">
								<i class="fab fa-github"></i>
							</span>
							<span>Code</span>
							</a>
						</span>

						<!-- Model Link. -->
						<span class="link-block">
							<a href="https://huggingface.co/KangarooGroup/kangaroo"
							class="external-link button is-normal is-rounded is-dark">
							<span class="icon">
								ðŸ¤—
							</span>
							<span>Model</span>
							</a>
						</span>
        	</div>
      	</div>
    	</div>
  	</div>
	</div>
</section>

<!-- Abstract. -->
<section class="section">
	<div class="container is-max-desktop">
		<div class="columns is-centered has-text-centered">
			<div class="column is-four-fifths">
				<h2 class="title is-3">Abstract</h2>
				<div class="content has-text-justified">
					<p>
						Rapid advancements have been made in extending Large Language Models (LLMs) to Large Multi-modal Models (LMMs). However, extending 
                        input modality of LLMs to video data remains a challenging endeavor, especially for long videos. Due to insufficient access to 
                        large-scale high-quality video data and the excessive compression of visual features, current methods exhibit limitations in effectively 
                        processing long videos. In this paper, we introduce Kangaroo, a powerful Video LMM aimed at addressing these challenges. Confronted 
                        with issue of inadequate training data, we develop a data curation system to build a large-scale dataset with high-quality annotations 
                        for visionlanguage pre-training and instruction tuning. In addition, we design a curriculum training pipeline with gradually increasing 
                        resolution and number of input frames to accommodate long videos. Evaluation results demonstrate that, with 8B parameters, Kangaroo 
                        achieves state-of-the-art performance across a variety of video understanding benchmarks while exhibiting competitive results on others.
                        Particularly, on benchmarks specialized for long videos, Kangaroo excels some larger models with over 10B parameters and proprietary models.
					</p>
                    <strong> Highlights: </strong>
                    <ol>
						<li><strong>Large-scale Data Curation.</strong>
                            Large-scale Data Curation. We develop a data curation system to generate captions for open-source and internal videos and construct a 
                            video instruction tuning dataset covering a variety of tasks.</li>
                        <li><strong>Long-context Video Input.</strong>
                            We extend the maximum frames of input videos to 160, with corresponding sequence length up to 22k tokens.</li>
                        <li><strong>Superior Performance.</strong>
                            Our model achieves state-of-the-art performance on the a variety of comprehensive benchmarks and outperforms some larger open-source 
                            models with over 10B parameters and proprietary models on certain benchmarks.</li>
                        <li><strong>Bilingual Conversation.</strong>
						    Our proposed model is equipped with the capability of Chinese, English and bilingual conversations, and support single/multi-round 
                            conversation paradigms.</li>
					</ol>
				</div>
			</div>
		</div>
</section>

<!-- Model -->
<section class="section">
	<div class="container is-max-desktop">
		<div class="columns is-centered has-text-centered">
			<div class="column is-four-fifths">
				<h2 class="title is-3" style="margin-bottom: 20px">Model</h2>
			</div>
		</div>
	</div>
	<div style="text-align: center;">
        <img id="model" width="60%" src="static/images/model.png">
        <h3 class="subtitle has-text-centered">
        <p style="font-family:Times New Roman; font-size: 18px;">Architecture of our Kangaroo model.</p>
	</div>
</section>

<!-- Dataset -->
<section class="section">
	<div class="container is-max-desktop">
		<div class="columns is-centered has-text-centered">
			<div class="column is-four-fifths">
				<h2 class="title is-3">Dataset Statistics</h2>
				<div class="content has-text-justified">
					<p><strong style="font-size: 20px;">Data Curation</strong><br>
						We select 300M images from LAION, COYO and Wukong for image-text alignment and 60M videos from Webvid, Youku-mPLUG and internal social media short 
                        videos for video-text alignment, following a category balance strategy. Videos with excessive text coverage, significant face coverage and low 
                        optical flow scores are filtered out. 
					</p>
					<p><strong style="font-size: 20px;">Automatic Annotation</strong><br>
						We develop an automatic video annotation system. The process begins with the extraction of five key frames of each video, followed by the utilization 
                        of three distinct off-the-shelf Multimodal Large Language Models (MLLM) to generate frame captions. Next, we employ an LLM to synthesize the key 
                        frame captions into a comprehensive video caption. We further curate a subset of 6M pre-training data and incorporate 900K dense caption data from 
                        ShareGPTVideo dataset for the refined pre-training stage.
					</p>
					<p><strong style="font-size: 20px;">Instruction Tuning Dataset</strong><br>
						To enhance the instruction following ability of the model, we compile a video instruction tuning dataset comprising 2.24M samples from public and 
                        internal sources. The dataset consists of short caption, detailed description, multi-choice/open-ended QA, single/multi-round conversation 
                        in both Chinese and English.
				</div>
			</div>
		</div>
		<div class="image-row" style="text-align: center;">
			<img src="static/images/instruction_dataset.png" width="35%" style="margin: 20px;">
			<h3 class="subtitle has-text-centered">
			<p style="font-family:Times New Roman; font-size: 18px;">Distribution of instruction tuning dataset.</p>
		</div>
	</div>
</section>

<!-- Model Card -->
<section class="section">
	<div class="container is-max-desktop">
	<div class="columns is-centered has-text-centered">
		<div class="column is-four-fifths">
			<h2 class="title is-3">Model Card</h2>
		</div>
	</div>
	</div>
	<div style="text-align: center; margin: 40px;">
		<table style="text-align: center;;margin:auto">
			<colgroup>
			<col style="mso-width-source:userset;width:150pt"> 
			<col style="mso-width-source:userset;width:160pt"> 
			<col style="mso-width-source:userset;width:100pt"> 
			<col style="mso-width-source:userset;width:180pt">
			<col style="mso-width-source:userset;width:120pt">
			</colgroup>
			<tbody>
			<tr height="45"> 
			<!-- <th rowspan="2" class="gg">Model</th>  -->
			<th rowspan="2" style="font-size: 20px;" class="gg">Modules</th> 
			<th class="gg">Vision Encoder</th> 
			<th class="gg">Projector</th> 
			<th class="gg">Patchify Module</th> 
			<th class="gg">LLM</th> 
			</tr> 
			<tr height="45"> 
			<td class="lgg">EVA-CLIP-L</td> 
			<td class="lgg">Linear</td> 
			<td class="lgg">3D Depthwise Convolution</td> 
			<td class="lgg">Llama3-8B-Instruct</td> 

			<tr height="45"> 
			<th colspan="1" rowspan="3" class="gg" style="font-size: 18px;">Stage 1<br>Image Pre-training</th> 
			<th class="wg">Resolution</th> 
			<td colspan="3" class="wg">224</td> 
			</tr> 
			<tr height="45"> 
			<th class="lgg">Training Data</th> 
			<td colspan="3" class="lgg">300M</td> 
			</tr> 
			<tr height="45"> 
			<th class="wg">Trainable Module</th> 
			<td colspan="3" class="wg">ViT + Projector</td> 
			</tr> 
			<tr height="45"> 
			<th colspan="1" rowspan="3" class="gg" style="font-size: 18px;">Stage 2<br>Video Pre-training</th> 
			<th class="lgg">Resolution</th> 
			<td colspan="3" class="lgg">224 Ã— 8 frames</td> 
			</tr> 
			<tr height="45"> 
			<th class="wg">Training Data</th> 
			<td colspan="3" class="wg">60M</td> 
			</tr> 
			<tr height="45"> 
			<th class="lgg">Trainable Module</th> 
			<td colspan="3" class="lgg">ViT + Projector</td> 
			</tr> 
			<tr height="45"> 
			<th colspan="1" rowspan="3" class="gg" style="font-size: 18px;">Stage 3<br>Pre-training Refinement</th> 
			<th class="wg">Resolution</th> 
			<td colspan="3" class="wg">448 Ã— 16 frames</td> 
			</tr> 
			<tr height="45"> 
			<th class="lgg">Training Data</th> 
			<td colspan="3" class="lgg">6.9M</td> 
			</tr> 
			<tr height="45"> 
			<th class="wg">Trainable Module</th> 
			<td colspan="3" class="wg">ViT + Projector + Patchify Module</td> 
			</tr> 
			<tr height="45"> 
			<th colspan="1" rowspan="3" class="gg" style="font-size: 18px;">Stage 4<br>Instruction Tuning</th> 
			<th class="lgg">Resolution</th> 
			<td colspan="3" class="lgg">448 Ã— (16 to 64 frames)</td> 
			</tr> 
			<tr height="45"> 
			<th class="wg">Training Data</th> 
			<td colspan="3" class="wg">2.24M</td> 
			</tr> 
			<tr height="45"> 
			<th class="lgg">Trainable Module</th> 
			<td colspan="3" class="lgg">Full model</td> 
			</tr> 
			<tr height="45"> 
			<th colspan="1" rowspan="3" class="gg" style="font-size: 18px;">Stage 5<br>Long Video Tuning</th> 
			<th class="wg">Resolution</th> 
			<td colspan="3" class="wg">448 Ã— (16 to 64 frames for short videos, 64 to 160 frames for long videos)</td> 
			</tr> 
			<tr height="45"> 
			<th class="lgg">Training Data</th> 
			<td colspan="3" class="lgg">700K</td> 
			</tr> 
			<tr height="45"> 
			<th class="wg">Trainable Module</th> 
			<td colspan="3" class="wg">Projector + Patchify Module + LLM</td> 
			</tr> <!--EndFragment--> 
			</tbody>
		</table>
	</div>
</section>

<!-- Result -->
<section class="section">
	<div class="container is-max-desktop">
		<div class="columns is-centered has-text-centered">
			<div class="column is-four-fifths">
				<h2 class="title is-3">Results</h2>
			</div>
		</div>
	</div>
	<div style="text-align: center;">
		<img src="static/images/bench.png" width="70%" style="margin-top: 40px;">
        <h3 class="subtitle has-text-centered">
        <p style="font-family:Times New Roman; font-size: 18px; margin-top: 10px;">Results on Comprehensive Video Understanding Benchmarks.</p>
	</div>
	<div style="text-align: center;">
		<img src="static/images/videomme.png" width="70%" style="margin-top: 40px;">
        <h3 class="subtitle has-text-centered">
        <p style="font-family:Times New Roman; font-size: 18px; margin-top: 10px;">Results on VideoMME.</p>
	</div>
    <div style="text-align: center;">
		<img src="static/images/seed.png" width="55%" style="margin-top: 40px;">
        <h3 class="subtitle has-text-centered">
        <p style="font-family:Times New Roman; font-size: 18px; margin-top: 10px;">Results on Seedbench-Video.</p>
	</div>
</section>

<!-- Demo -->
<section class="section">
	<div style="text-align: center;">
		<h2 class="title is-3">Demo</h2>
		<p style="font-family:Times New Roman;font-size:larger;">
		(Demo videos are from <a href=https://openai.com/index/video-generation-models-as-world-simulators>openai sora)</a></p>
		
		<!-- Demo1 -->
        <div class="video-container" style="text-align: center; margin: 50px;">
			<video id="demo1" controls muted loop playsinline style="height: 250px;">
				<source src="./static/videos/demo1.mp4" type="video/mp4">
			</video>
		</div>
		<img id="demo1" width="70%" src="static/images/demo1.png">
		
	</div>

	<!-- Demo2 -->
	<div style="text-align: center; margin: 100px;">
		<div class="video-container" style="text-align: center; margin: 10px;">
			<video id="demo2_1" controls muted loop playsinline style="height: 200px; margin: 20px">
				<source src="./static/videos/demo2_1.mp4" type="video/mp4">
			</video>
			<video id="demo2_2" controls muted loop playsinline style="height: 200px; margin: 20px">
				<source src="./static/videos/demo2_2.mp4" type="video/mp4">
			</video>
		</div>
		<img id="demo2" width="85%" src="static/images/demo2.png">
	</div>

    <!-- Demo3 -->
	<div style="text-align: center; margin: 100px;">
		<img id="demo3" width="85%" src="static/images/demo3.png">
	</div>
</section>

<!-- Citation -->
<section class="section">
	<h2 class="title is-3" style="text-align: center;">Citation</h2>
    <pre style="width: 850px; margin: auto;"><code>
        @misc{kangaroogroup,
            title={Kangaroo: A Powerful Video-Language Model Supporting Long-context Video Input},
            url={https://kangaroogroup.github.io/Kangaroo.github.io/},
            author={Jiajun Liu and Yibing Wang and Hanghang Ma and Xiaoping Wu and Xiaoqi Ma and Jie Hu},
            month={July},
            year={2024}
        }
    </code></pre>
</section>

<!-- foot -->
<footer class="footer">
	<div class="columns is-centered">
		<div class="column is-4">
			<div class="content">
				<p style="font-size: larger;"><strong>Acknowledgments:</strong></p>
				<p>
					This website is adopted from the <a href="https://github.com/nerfies/nerfies.github.io">
					source code </a> of this website, licensed under a <a rel="license" href="http://creativecommons.org/licenses/by-sa/4.0/">
					Creative Commons Attribution-ShareAlike 4.0 International License</a>. Thanks for their excellent work.
				</p>
			</div>
		</div>
	</div>
</footer>

</body>
</html>
